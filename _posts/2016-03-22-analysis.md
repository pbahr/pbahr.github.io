---
layout: post
title: "Titanic Survival Analysis"
author: "Payam Bahreyni"
date: 2016-03-22
categories: [tutorials]
tags : [Machine Learning]
---

## Loading Data & Initial Analysis




{% highlight r linenos %}
source("library.R")
{% endhighlight %}


{% highlight r linenos %}
# Load data
train.data <- read.csv("input/train.csv", stringsAsFactors = F)
test.data <- read.csv("input/test.csv", stringsAsFactors = F)

train.outcome <- train.data$Survived
train.outcome <- factor(train.outcome)

# Remove "Survived" column
train.data <- train.data[,-2]

train.len <- nrow(train.data)
test.len <- nrow(test.data)

# Combine training & testing data
full.data <- rbind(train.data, test.data)

# Turn categorical vars into factor
full.data$Pclass <- factor(full.data$Pclass)
full.data$Sex <- factor(full.data$Sex)
full.data$Embarked <- factor(full.data$Embarked)

str(full.data)
{% endhighlight %}



{% highlight text %}
## 'data.frame':	1309 obs. of  11 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Pclass     : Factor w/ 3 levels "1","2","3": 3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : chr  "Braund, Mr. Owen Harris" "Cumings, Mrs. John Bradley (Florence Briggs Thayer)" "Heikkinen, Miss. Laina" "Futrelle, Mrs. Jacques Heath (Lily May Peel)" ...
##  $ Sex        : Factor w/ 2 levels "female","male": 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : chr  "A/5 21171" "PC 17599" "STON/O2. 3101282" "113803" ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  "" "C85" "" "C123" ...
##  $ Embarked   : Factor w/ 4 levels "","C","Q","S": 4 2 4 4 4 3 4 4 4 2 ...
{% endhighlight %}



{% highlight r linenos %}
# Split data back into training and testing
train.data <- full.data[1:train.len, ]
test.data <- full.data[(train.len+1):nrow(full.data), ]

sapply(train.data[, -c(1, 3, 8, 10)], summary)
{% endhighlight %}



{% highlight text %}
## $Pclass
##   1   2   3 
## 216 184 491 
## 
## $Sex
## female   male 
##    314    577 
## 
## $Age
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##    0.42   20.12   28.00   29.70   38.00   80.00     177 
## 
## $SibSp
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   0.000   0.523   1.000   8.000 
## 
## $Parch
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.3816  0.0000  6.0000 
## 
## $Fare
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00    7.91   14.45   32.20   31.00  512.30 
## 
## $Embarked
##       C   Q   S 
##   2 168  77 644
{% endhighlight %}



{% highlight r linenos %}
train.data$Survived <- train.outcome
{% endhighlight %}

### Findings 

* Most people are travelled in Class 3.
* Most passengers were male.
* For the passengers with Age specified, 50% are older than 28. We have a lot of missing values in Age variable.
* Most passengers did not travel with their spouse or siblings on board. 
* Most passengers do not have their parents and children on board.
* The 75% of the passengers paid less than $31 for fare, and the maximum fare paid was $512.
* Most passengers came on board on the port specified by "S".

## EDA

Let's take a look at different groups and their survival rate.


{% highlight r linenos %}
prop.table(table(train.data$Sex, useNA = "ifany"))
{% endhighlight %}



{% highlight text %}
## 
##   female     male 
## 0.352413 0.647587
{% endhighlight %}



{% highlight r linenos %}
prop.table(table(train.data$Pclass, useNA = "ifany"))
{% endhighlight %}



{% highlight text %}
## 
##         1         2         3 
## 0.2424242 0.2065095 0.5510662
{% endhighlight %}

Majority of passengers are men (65%) and passengers have 3 different classes, 1 (24%), 2 (21%), or 3 (55%).

### Survival Rate


{% highlight r linenos %}
table(train.data$Survived, useNA = "ifany")
{% endhighlight %}



{% highlight text %}
## 
##   0   1 
## 549 342
{% endhighlight %}



{% highlight r linenos %}
prop.table(table(train.data$Survived, useNA = "ifany"))
{% endhighlight %}



{% highlight text %}
## 
##         0         1 
## 0.6161616 0.3838384
{% endhighlight %}



{% highlight r linenos %}
table(train.data$Sex, train.data$Survived)
{% endhighlight %}



{% highlight text %}
##         
##            0   1
##   female  81 233
##   male   468 109
{% endhighlight %}



{% highlight r linenos %}
prop.table(table(train.data$Sex, train.data$Survived), margin = 1)
{% endhighlight %}



{% highlight text %}
##         
##                  0         1
##   female 0.2579618 0.7420382
##   male   0.8110919 0.1889081
{% endhighlight %}



{% highlight r linenos %}
prop.table(table(train.data$Pclass, train.data$Survived), margin = 1)
{% endhighlight %}



{% highlight text %}
##    
##             0         1
##   1 0.3703704 0.6296296
##   2 0.5271739 0.4728261
##   3 0.7576375 0.2423625
{% endhighlight %}

Most people didn't survive (62%). But the survival rate is not the same across different groups. Females had higher chance of survival, 74% as compared to 19% for men. Class 1 passengers had 63% chance of survival, compared to 47% and 24% for class 2 and 3, respectively.


{% highlight r linenos %}
prop.table(table(train.data$Pclass, train.data$Sex), margin = 1)
{% endhighlight %}



{% highlight text %}
##    
##        female      male
##   1 0.4351852 0.5648148
##   2 0.4130435 0.5869565
##   3 0.2932790 0.7067210
{% endhighlight %}



{% highlight r linenos %}
prop.table(table(train.data$Pclass, train.data$Sex, train.data$Survived), margin = 3)
{% endhighlight %}



{% highlight text %}
## , ,  = 0
## 
##    
##          female        male
##   1 0.005464481 0.140255009
##   2 0.010928962 0.165755920
##   3 0.131147541 0.546448087
## 
## , ,  = 1
## 
##    
##          female        male
##   1 0.266081871 0.131578947
##   2 0.204678363 0.049707602
##   3 0.210526316 0.137426901
{% endhighlight %}

The survival rate in different classes may have some relationship to percentage of women in those classes.


{% highlight r linenos %}
round(train.len * 100/(train.len + test.len), digits = 2)
{% endhighlight %}



{% highlight text %}
## [1] 68.07
{% endhighlight %}



{% highlight r linenos %}
round(test.len * 100/(train.len + test.len), digits = 2)
{% endhighlight %}



{% highlight text %}
## [1] 31.93
{% endhighlight %}

The data is partitioned into 68% for training dataset and 32% for test data set.

## Graphical Analysis

{% highlight r linenos %}
# Graphical
library(ggplot2)
library(scales)

ggplot(train.data, aes(Sex, fill= Survived)) +
    geom_bar(stat = "bin", position = "stack") +
#    geom_text(stat = "bin", aes(label= ..count.., vjust= 1),
#              position = position_stack()) +
    ggtitle("Women have higher chance of survival")
{% endhighlight %}

![plot of chunk unnamed-chunk-7](/figure/source/2016-03-22-analysis/unnamed-chunk-7-1.png) 

{% highlight r linenos %}
ggplot(train.data, aes(x=Pclass, fill= Survived)) +
    geom_bar(stat = "bin", position = "stack") +
    ggtitle("In class 1 most people survived and in class 3 most did not")
{% endhighlight %}

![plot of chunk unnamed-chunk-7](/figure/source/2016-03-22-analysis/unnamed-chunk-7-2.png) 

{% highlight r linenos %}
ggplot(train.data, aes(x=Pclass, fill= Survived))+
    geom_bar(stat = "bin", position = "stack") +
    facet_wrap(~Sex) +
    ggtitle("Most women survive across passenger classes")
{% endhighlight %}

![plot of chunk unnamed-chunk-7](/figure/source/2016-03-22-analysis/unnamed-chunk-7-3.png) 

{% highlight r linenos %}
ggplot(train.data, aes(y=Fare, x= Survived)) +
    geom_boxplot() +
    facet_wrap(~Pclass, ncol = 3, scales = "free") +
    ggtitle("People who survived paid higher fare on average")
{% endhighlight %}

![plot of chunk unnamed-chunk-7](/figure/source/2016-03-22-analysis/unnamed-chunk-7-4.png) 

## Statistical Analysis

We saw that fare may have some effect on the survival rate. Let's see if the effect is real.


{% highlight r linenos %}
aggregate(Fare ~ Survived + Pclass, data=train.data, mean)
{% endhighlight %}



{% highlight text %}
##   Survived Pclass     Fare
## 1        0      1 64.68401
## 2        1      1 95.60803
## 3        0      2 19.41233
## 4        1      2 22.05570
## 5        0      3 13.66936
## 6        1      3 13.69489
{% endhighlight %}



{% highlight r linenos %}
with(train.data, t.test(Fare[Pclass == 1 & Survived == 1], 
                        Fare[Pclass == 1 & Survived == 0]))
{% endhighlight %}



{% highlight text %}
## 
## 	Welch Two Sample t-test
## 
## data:  Fare[Pclass == 1 & Survived == 1] and Fare[Pclass == 1 & Survived == 0]
## t = 3.1004, df = 206.3, p-value = 0.002202
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  11.25978 50.58826
## sample estimates:
## mean of x mean of y 
##  95.60803  64.68401
{% endhighlight %}



{% highlight r linenos %}
with(train.data, t.test(Fare[Pclass == 2 & Survived == 1], 
                        Fare[Pclass == 2 & Survived == 0]))
{% endhighlight %}



{% highlight text %}
## 
## 	Welch Two Sample t-test
## 
## data:  Fare[Pclass == 2 & Survived == 1] and Fare[Pclass == 2 & Survived == 0]
## t = 1.3615, df = 173.08, p-value = 0.1751
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -1.188767  6.475511
## sample estimates:
## mean of x mean of y 
##  22.05570  19.41233
{% endhighlight %}



{% highlight r linenos %}
with(train.data, t.test(Fare[Pclass == 3 & Survived == 1], 
                        Fare[Pclass == 3 & Survived == 0]))
{% endhighlight %}



{% highlight text %}
## 
## 	Welch Two Sample t-test
## 
## data:  Fare[Pclass == 3 & Survived == 1] and Fare[Pclass == 3 & Survived == 0]
## t = 0.021921, df = 222.91, p-value = 0.9825
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.268933  2.319980
## sample estimates:
## mean of x mean of y 
##  13.69489  13.66936
{% endhighlight %}

So, we see that in class 1, the difference in fare between passengers who survived and who didn't is statistiscally significant. Maybe the rich found a way to buy lifeboats :).

## Machine Learning

Let's do some modeling to see how we can predict the survival rate given other variables about the passengers.

{% highlight r linenos %}
library(caret)
{% endhighlight %}



{% highlight text %}
## Warning: package 'caret' was built under R version 3.2.3
{% endhighlight %}



{% highlight text %}
## Loading required package: lattice
{% endhighlight %}



{% highlight r linenos %}
library(rpart)

sex.model.tree <- train(Survived ~ Sex + Pclass, data= train.data, method= "rpart")
sex.class.survival <- predict(sex.model.tree, train.data)
confusionMatrix(sex.class.survival, train.data$Survived)
{% endhighlight %}



{% highlight text %}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 468 109
##          1  81 233
##                                           
##                Accuracy : 0.7868          
##                  95% CI : (0.7584, 0.8132)
##     No Information Rate : 0.6162          
##     P-Value [Acc > NIR] : < 2e-16         
##                                           
##                   Kappa : 0.5421          
##  Mcnemar's Test P-Value : 0.05014         
##                                           
##             Sensitivity : 0.8525          
##             Specificity : 0.6813          
##          Pos Pred Value : 0.8111          
##          Neg Pred Value : 0.7420          
##              Prevalence : 0.6162          
##          Detection Rate : 0.5253          
##    Detection Prevalence : 0.6476          
##       Balanced Accuracy : 0.7669          
##                                           
##        'Positive' Class : 0               
## 
{% endhighlight %}



{% highlight r linenos %}
plot.decision.tree(sex.model.tree$finalModel)
{% endhighlight %}

![plot of chunk unnamed-chunk-9](/figure/source/2016-03-22-analysis/unnamed-chunk-9-1.png) 

Although `Sex` and `Pclass` looked like good predictors for `Survival`, `Pclass` didn't get picked up by `rpart` as it didn't add much information to the decision tree.


{% highlight r linenos %}
sex.class.fare.tree <- train(Survived ~ Sex + Pclass + Fare, data= train.data, method= "rpart")
sex.class.fare.survival <- predict(sex.class.fare.tree, train.data)
confusionMatrix(sex.class.fare.survival, train.data$Survived)
{% endhighlight %}



{% highlight text %}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 492 112
##          1  57 230
##                                          
##                Accuracy : 0.8103         
##                  95% CI : (0.783, 0.8356)
##     No Information Rate : 0.6162         
##     P-Value [Acc > NIR] : < 2.2e-16      
##                                          
##                   Kappa : 0.5865         
##  Mcnemar's Test P-Value : 3.269e-05      
##                                          
##             Sensitivity : 0.8962         
##             Specificity : 0.6725         
##          Pos Pred Value : 0.8146         
##          Neg Pred Value : 0.8014         
##              Prevalence : 0.6162         
##          Detection Rate : 0.5522         
##    Detection Prevalence : 0.6779         
##       Balanced Accuracy : 0.7843         
##                                          
##        'Positive' Class : 0              
## 
{% endhighlight %}



{% highlight r linenos %}
plot.decision.tree(sex.class.fare.tree$finalModel)
{% endhighlight %}

![plot of chunk unnamed-chunk-10](/figure/source/2016-03-22-analysis/unnamed-chunk-10-1.png) 

As expected, a mix of passenger class and fare plays a role in our prediction this time. We used to predict that women survive with a 74% chance. Now, we can be more accurate. Women have a 95% chance of survival if not in class 3. From class 3 women, 11% of people who paid fare of more than $23, and 59% of people who paid less survived. Let's see if it makes sense.


{% highlight r linenos %}
ggplot(train.data[train.data$Pclass== "3" & train.data$Sex == "female",], aes(x= Survived, y= Fare)) +
    geom_boxplot() +
    ggtitle("Class 3 women who survived paid less fare on average")
{% endhighlight %}

![plot of chunk unnamed-chunk-11](/figure/source/2016-03-22-analysis/unnamed-chunk-11-1.png) 

{% highlight r linenos %}
without.age.tree <- train(Survived ~ Sex + Pclass + Fare + SibSp + Parch + Embarked,
                          data= train.data,
                          method= "rpart")
plot.decision.tree(without.age.tree$finalModel)
{% endhighlight %}

![plot of chunk unnamed-chunk-11](/figure/source/2016-03-22-analysis/unnamed-chunk-11-2.png) 

Adding all the variables but age didn't change the decision tree.

### Method 2: Logistic Regression


{% highlight r linenos %}
sex.class.fare.logit <- glm(Survived ~ Sex + Pclass + Fare, 
                            family = "binomial", data= train.data)
sex.class.fare.survival <- predict(sex.class.fare.logit, train.data, type = "response")
head(sex.class.fare.survival)
{% endhighlight %}



{% highlight text %}
##          1          2          3          4          5          6 
## 0.09498249 0.90695089 0.59184845 0.90412750 0.09510741 0.09517122
{% endhighlight %}



{% highlight r linenos %}
summary(sex.class.fare.logit)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## glm(formula = Survived ~ Sex + Pclass + Fare, family = "binomial", 
##     data = train.data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2395  -0.7285  -0.4470   0.6468   2.1753  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  2.147554   0.278322   7.716 1.20e-14 ***
## Sexmale     -2.624648   0.185217 -14.171  < 2e-16 ***
## Pclass2     -0.736578   0.271056  -2.717  0.00658 ** 
## Pclass3     -1.790329   0.251758  -7.111 1.15e-12 ***
## Fare         0.001815   0.002102   0.864  0.38779    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1186.66  on 890  degrees of freedom
## Residual deviance:  826.12  on 886  degrees of freedom
## AIC: 836.12
## 
## Number of Fisher Scoring iterations: 4
{% endhighlight %}



{% highlight r linenos %}
without.age.logit <- glm(Survived ~ Sex + Pclass + Fare + SibSp + Parch + Embarked, 
                            family = "binomial", data= train.data)
without.age.survival <- predict(without.age.logit, train.data, type = "response")
head(without.age.survival)
{% endhighlight %}



{% highlight text %}
##          1          2          3          4          5          6 
## 0.07711117 0.92700344 0.61406071 0.87823208 0.09544919 0.12666434
{% endhighlight %}



{% highlight r linenos %}
summary(without.age.logit)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## glm(formula = Survived ~ Sex + Pclass + Fare + SibSp + Parch + 
##     Embarked, family = "binomial", data = train.data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3283  -0.7239  -0.4477   0.6232   2.5661  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  14.320071 624.193859   0.023   0.9817    
## Sexmale      -2.713643   0.198644 -13.661  < 2e-16 ***
## Pclass2      -0.535366   0.282519  -1.895   0.0581 .  
## Pclass3      -1.603425   0.270920  -5.918 3.25e-09 ***
## Fare          0.003075   0.002489   1.236   0.2166    
## SibSp        -0.230956   0.101489  -2.276   0.0229 *  
## Parch        -0.076884   0.114612  -0.671   0.5023    
## EmbarkedC   -11.766763 624.193892  -0.019   0.9850    
## EmbarkedQ   -11.959792 624.193944  -0.019   0.9847    
## EmbarkedS   -12.276601 624.193882  -0.020   0.9843    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1186.66  on 890  degrees of freedom
## Residual deviance:  810.58  on 881  degrees of freedom
## AIC: 830.58
## 
## Number of Fisher Scoring iterations: 13
{% endhighlight %}

## Feature Creation

{% highlight r linenos %}
# Create the column child, and indicate whether child or no child
train2 <- train.data
train2$Child <- NA
train2$Child <- ifelse(is.na(train2$Age), NA, ifelse(train2$Age < 18, 1, 0))
train2$Child <- factor(train2$Child)

summary(train2$Age)
round(sum(is.na(train2$Age))* 100/nrow(train2), digits = 2)

train2$AgeGroup <- cut(train2$Age, breaks=c(0, 5, 10, 18, 28, 38, 80),
                       labels=c("<5", "5-10", "10-18", "18-28", "28-38", ">38"))
prop.table(table(train2$AgeGroup, useNA = "ifany"))

# Two-way comparison
prop.table(table(train2$Child, train2$Survived), margin= 1)
# Result: There is a difference in survival rate whether child or not

ggplot(train2, aes(x=Child, fill= Survived)) +
    geom_bar(stat = "bin", position = "dodge")

ggplot(train2, aes(x= Sex, y= AgeGroup, color= Survived)) +
    geom_jitter()

prop.table(table(train2$Sex, train2$AgeGroup), margin = 1)

aggregate(Survived ~ Sex + AgeGroup, data= train2, function(x) {
    sum(x$Survived == "1")/nrow(x)
})
{% endhighlight %}

